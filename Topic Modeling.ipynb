{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "This is the course material for 'Topic Modeling'. The notebook is prepared in Python 3.7+.\n",
    "\n",
    "Author: Avinash OK ( okavinashok@gmail.com )\n",
    "\n",
    "In this notebook we try to give you answers to the following questions:\n",
    "\n",
    "* What is Topic Modeling?\n",
    "* What are the different steps involved in the process?\n",
    "* What all approaches can be used for Topic Modeling?\n",
    "* When should you use Topic Modeling?\n",
    "* How can this Notebook help you to build a Topic Model from scratch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all Required Libraries\n",
    "import nltk, string, re, os, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "%matplotlib inline\n",
    "\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "logging.getLogger(\"lda\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents.\n",
    "\n",
    "\n",
    "Contents covered in this notebook:\n",
    "1. Explanatory Data Analysis \n",
    "2. Text Processing  \n",
    "    2.1. Tokenizing and  tf-idf algorithm  \n",
    "    2.2. K-means Clustering  \n",
    "    2.3. Latent Dirichlet Allocation (LDA)  / Topic Modelling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Explanatory Data Analysis \n",
    "\n",
    "Here, we choose a very popular NLP dataset [`20 News Groups`](http://qwone.com/~jason/20Newsgroups/). It can be downloaded from this [link](https://www.kaggle.com/crawford/20-newsgroups/download) or imported directly from Scikit learn, a popular Data Science library in Python.\n",
    "\n",
    "#####  What is 20 News Groups?\n",
    "- The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. It was originally collected by Ken Lang,  for his Newsweeder. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\n",
    "\n",
    "##### **Here is a list of the 20 newsgroups, partitioned according to subject matter:**\n",
    "<img src=\"./resources/20NewsGroup.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "df = pd.DataFrame([newsgroups_train.data, newsgroups_train.target.tolist()]).T\n",
    "df.columns = ['text', 'target']\n",
    "\n",
    "targets = pd.DataFrame( newsgroups_train.target_names)\n",
    "targets.columns=['title']\n",
    "\n",
    "dataframe = pd.merge(df, targets, left_on='target', right_index=True)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some sample texts from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text, topic in zip(dataframe['text'][125:130], dataframe['title'][125:130]):\n",
    "    print(\"#\"*125)\n",
    "    print(\"Topic: \"+ topic) \n",
    "    print(\"Text: \"+ text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic level split in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataframe['title'].value_counts().index.values.astype('str')\n",
    "y = dataframe['title'].value_counts().values\n",
    "pct = [(\"%.2f\"%(v*100))+\"%\"for v in (y/len(dataframe))]\n",
    "\n",
    "trace1 = go.Bar(x=x, y=y, text=pct,\n",
    "                marker=dict(\n",
    "                color = y,colorscale='Portland',showscale=True,\n",
    "                reversescale = False\n",
    "                ))\n",
    "layout = dict(title= 'Topic Level split in the dataset',\n",
    "              yaxis = dict(title='Count'),\n",
    "              xaxis = dict(title='Titles'))\n",
    "fig=dict(data=[trace1], layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our Game-Plan\n",
    "\n",
    "Now that we have an intial understanding of the dataset, let's quickly move on to the meat of the problem. For now, let's assume that we don't have the column `title` and try to see if we can cluster various documents in the column `text` based on similar words present. This is our \"focused column\" through out this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Text pre-processing\n",
    "In this stage, we perform a basic preprocessing over our focussed column `text`.\n",
    "\n",
    "It will be slightly challenging to parse through this column since it's unstructured data. As a part of text preprocessing, we will strip out all punctuations, remove some english stop words (i.e. redundant words such as \"a\", \"the\", etc.) and any other words with a length less than 3.\n",
    "\n",
    "<img src=\"./resources/TopicModelingPreprocessing.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    sent_tokenize(): segment text into sentences\n",
    "    word_tokenize(): break sentences into words\n",
    "    \"\"\"\n",
    "    try: \n",
    "        regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
    "        text = regex.sub(\" \", text) # remove punctuation\n",
    "        \n",
    "        tokens_ = [word_tokenize(s) for s in sent_tokenize(text)]\n",
    "        tokens = []\n",
    "        for token_by_sent in tokens_:\n",
    "            tokens += token_by_sent\n",
    "        tokens = list(filter(lambda t: t.lower() not in stop, tokens))\n",
    "        filtered_tokens = [w for w in tokens if re.search('[a-zA-Z]', w)]\n",
    "        filtered_tokens = [w.lower() for w in filtered_tokens if len(w)>=3]\n",
    "        \n",
    "        return filtered_tokens\n",
    "            \n",
    "    except TypeError as e: print(text,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of words for each category\n",
    "cat_desc = dict()\n",
    "for cat in newsgroups_train.target_names: \n",
    "    text = \" \".join(dataframe.loc[dataframe['title']==cat, 'text'].values)\n",
    "    cat_desc[cat] = tokenize(text)\n",
    "\n",
    "# flat list of all words combined\n",
    "flat_lst = [item for sublist in list(cat_desc.values()) for item in sublist]\n",
    "allWordsCount = Counter(flat_lst)\n",
    "all_top10 = allWordsCount.most_common(20)\n",
    "x = [w[0] for w in all_top10]\n",
    "y = [w[1] for w in all_top10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataframe['title'].value_counts().values\n",
    "pct = [(\"%.2f\"%(v*100))+\"%\"for v in (y/len(dataframe))]\n",
    "\n",
    "trace1 = go.Bar(x=x, y=y, text=pct)\n",
    "layout = dict(title= 'Overall Word Frequency in the dataset',\n",
    "              yaxis = dict(title='Count'),\n",
    "              xaxis = dict(title='Word'))\n",
    "fig=dict(data=[trace1], layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.a Tokenizing\n",
    "\n",
    "Most of the time, the first steps of an NLP project is to **\"tokenize\"** your documents, whose main purpose is to normalize our texts. The three fundamental stages will usually include: \n",
    "* break the descriptions into sentences and then break the sentences into tokens\n",
    "* remove punctuation and stop words\n",
    "* lowercase the tokens\n",
    "* herein, We can also only consider words that have length equal to or greater than 3 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# apply the tokenizer into the \"text\" column\n",
    "dataframe['tokens'] = dataframe['text'].map(tokenize)\n",
    "dataframe['tokens'] = dataframe['text'].map(tokenize)\n",
    "\n",
    "dataframe.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"Let's eyeball how the sentences have been tokenized:\")\n",
    "\n",
    "for description, tokens in zip(dataframe['text'].head(), dataframe['tokens'].head()):\n",
    "    print('description:', description)\n",
    "    print('tokens:', tokens)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could aso use the package `WordCloud` to easily visualize which words has the highest frequencies within each title:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dictionary with key=title and values as all the descriptions related.\n",
    "cat_desc = dict()\n",
    "for cat in newsgroups_train.target_names: \n",
    "    text = \" \".join(dataframe.loc[dataframe['title']==cat, 'text'].values)\n",
    "    cat_desc[cat] = tokenize(text)\n",
    "\n",
    "\n",
    "# find the most common words for the top 4 categories\n",
    "autos100 = Counter(cat_desc['rec.autos']).most_common(100)\n",
    "space100 = Counter(cat_desc['sci.space']).most_common(100)\n",
    "christian100 = Counter(cat_desc['soc.religion.christian']).most_common(100)\n",
    "mideast100 = Counter(cat_desc['talk.politics.mideast']).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordcloud(tup):\n",
    "    wordcloud = WordCloud(background_color='white',\n",
    "                          max_words=50, max_font_size=40,\n",
    "                          random_state=42\n",
    "                         ).generate(str(tup))\n",
    "    return wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(2, 2, figsize=(30, 15))\n",
    "\n",
    "ax = axes[0, 0]\n",
    "ax.imshow(generate_wordcloud(autos100), interpolation=\"bilinear\")\n",
    "ax.axis('off')\n",
    "ax.set_title(\"Title: Automobile; Top: 100\", fontsize=30)\n",
    "\n",
    "ax = axes[0, 1]\n",
    "ax.imshow(generate_wordcloud(space100))\n",
    "ax.axis('off')\n",
    "ax.set_title(\"Title: Space; Top: 100\", fontsize=30)\n",
    "\n",
    "ax = axes[1, 0]\n",
    "ax.imshow(generate_wordcloud(christian100))\n",
    "ax.axis('off')\n",
    "ax.set_title(\"Title: Christian; Top: 100\", fontsize=30)\n",
    "\n",
    "ax = axes[1, 1]\n",
    "ax.imshow(generate_wordcloud(mideast100))\n",
    "ax.axis('off')\n",
    "ax.set_title(\"Title: Mid-East; Top: 100\", fontsize=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.b tf-idf algorithm\n",
    "\n",
    "tf-idf is the acronym for **Term Frequency–inverse Document Frequency**. It quantifies the importance of a particular word in relative to the vocabulary of a collection of documents or corpus. The metric depends on two factors: \n",
    "- **Term Frequency**: the occurences of a word in a given document (i.e. bag of words)\n",
    "- **Inverse Document Frequency**: the reciprocal number of times a word occurs in a corpus of documents\n",
    "\n",
    "Think about of it this way: If the word is used extensively in all documents, its existence within a specific document will not be able to provide us much specific information about the document itself. So the second term could be seen as a penalty term that penalizes common words such as \"a\", \"the\", \"and\", etc. tf-idf can therefore, be seen as a weighting scheme for words relevancy in a specific document. For more info refer this [link](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=10, max_features=180000, tokenizer=tokenize, ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_desc = dataframe['text'].values\n",
    "vz = vectorizer.fit_transform(list(all_desc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vz is a tfidf matrix where:\n",
    "* the number of rows is the total number of descriptions\n",
    "* the number of columns is the total number of unique tokens across the descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create a dictionary mapping the tokens to their tfidf values\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "tfidf = pd.DataFrame(columns=['idf']).from_dict( dict(tfidf), orient='index')\n",
    "tfidf.columns = ['idf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the 10 tokens with the **lowest IDF score**, which is unsurprisingly, very generic words that we could not use to distinguish one description from another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.sort_values(by=['idf'], ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the 10 tokens with the **highest IDF score**, which includes words that are a lot specific that by looking at them, we could guess the categories that they belong to: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.sort_values(by=['idf'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the high dimension of our tfidf matrix, we need to reduce their dimension using the Singular Value Decomposition (SVD) technique. And to visualize our vocabulary, we could next use t-SNE to reduce the dimension from 50 to 2. t-SNE is more suitable for dimensionality reduction to 2 or 3. \n",
    "\n",
    "#### **t-Distributed Stochastic Neighbor Embedding (t-SNE)**\n",
    "\n",
    "t-SNE is a technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets. The goal is to take a set of points in a high-dimensional space and find a representation of those points in a lower-dimensional space, typically the 2D plane. It is based on probability distributions with random walk on neighborhood graphs to find the structure within the data. But since t-SNE complexity is significantly high, usually we'd use other high-dimension reduction techniques before applying t-SNE.\n",
    "\n",
    "First, let's take a sample from the our focused column `text` since t-SNE can take a very long time to execute. We can then reduce the dimension of each vector from to n_components (30) using SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sz = 10000\n",
    "\n",
    "dataframe_sample = dataframe.sample(n=sample_sz)\n",
    "vz_sample = vectorizer.fit_transform(list(dataframe_sample['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "n_comp= 30\n",
    "svd = TruncatedSVD(n_components=n_comp, random_state=42)\n",
    "svd_tfidf = svd.fit_transform(vz_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=42, n_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_tfidf = tsne_model.fit_transform(svd_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now possible to visualize our data points. Note that the deviation as well as the size of the clusters imply little information  in t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()\n",
    "plot_tfidf = bp.figure(plot_width=700, plot_height=600, title=\"tf-idf clustering of 'text'\", tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\", x_axis_type=None, y_axis_type=None, min_border=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_sample.reset_index(inplace=True, drop=True)\n",
    "\n",
    "tfidf_df = pd.DataFrame(tsne_tfidf, columns=['x', 'y'])\n",
    "tfidf_df['text'] = dataframe_sample['text']\n",
    "tfidf_df['tokens'] = dataframe_sample['tokens']\n",
    "tfidf_df['title'] = dataframe_sample['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tfidf.scatter(x='x', y='y', source=tfidf_df, alpha=0.7)\n",
    "hover = plot_tfidf.select(dict(type=HoverTool))\n",
    "hover.tooltips={\"text\": \"@text\", \"tokens\": \"@tokens\", \"title\":\"@title\"}\n",
    "show(plot_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "Video(\"./resources/Topic_model_scheme.webm\")\n",
    "# Video(\"https://upload.wikimedia.org/wikipedia/commons/7/70/Topic_model_scheme.webm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. K-means Clustering\n",
    "\n",
    "K-means clustering obejctive is to minimize the average squared Euclidean distance of the document / text from their cluster centroids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "num_clusters = 20 # need to be selected wisely\n",
    "kmeans_model = MiniBatchKMeans(n_clusters=num_clusters, init='k-means++', n_init=1, init_size=1000, batch_size=1000, verbose=0, max_iter=1000)\n",
    "\n",
    "kmeans = kmeans_model.fit(vz)\n",
    "kmeans_clusters = kmeans.predict(vz)\n",
    "kmeans_distances = kmeans.transform(vz)\n",
    "\n",
    "sorted_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    aux = ''\n",
    "    for j in sorted_centroids[i, :10]:\n",
    "        try:\n",
    "            aux += terms[j] + ' | '\n",
    "        except:\n",
    "            pass\n",
    "    print(aux)\n",
    "    print() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to plot these clusters, first we will need to reduce the dimension of the distances to 2 using tsne: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat the same steps for the sample\n",
    "kmeans = kmeans_model.fit(vz_sample)\n",
    "kmeans_clusters = kmeans.predict(vz_sample)\n",
    "kmeans_distances = kmeans.transform(vz_sample)\n",
    "# reduce dimension to 2 using tsne\n",
    "tsne_kmeans = tsne_model.fit_transform(kmeans_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = np.array([\"#6d8dca\", \"#69de53\", \"#723bca\", \"#c3e14c\", \"#c84dc9\", \"#68af4e\", \"#6e6cd5\",\n",
    "\"#e3be38\", \"#4e2d7c\", \"#5fdfa8\", \"#d34690\", \"#3f6d31\", \"#d44427\", \"#7fcdd8\", \"#cb4053\", \"#5e9981\",\n",
    "\"#803a62\", \"#9b9e39\", \"#c88cca\", \"#e1c37b\", \"#34223b\", \"#bdd8a3\", \"#6e3326\", \"#cfbdce\", \"#d07d3c\",\n",
    "\"#52697d\", \"#194196\", \"#d27c88\", \"#36422b\", \"#b68f79\"])\n",
    "\n",
    "#combined_sample.reset_index(drop=True, inplace=True)\n",
    "kmeans_df = pd.DataFrame(tsne_kmeans, columns=['x', 'y'])\n",
    "kmeans_df['cluster'] = kmeans_clusters\n",
    "kmeans_df['text'] = dataframe_sample['text']\n",
    "kmeans_df['title'] = dataframe_sample['title']\n",
    "\n",
    "plot_kmeans = bp.figure(plot_width=700, plot_height=600, title=\"KMeans clustering of 'text'\", tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\", x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "source = ColumnDataSource(data=dict(x=kmeans_df['x'], y=kmeans_df['y'], color=colormap[kmeans_clusters], text=kmeans_df['text'], title=kmeans_df['title'], cluster=kmeans_df['cluster']))\n",
    "\n",
    "plot_kmeans.scatter(x='x', y='y', color='color', source=source)\n",
    "hover = plot_kmeans.select(dict(type=HoverTool))\n",
    "hover.tooltips={\"text\": \"@text\", \"title\": \"@title\", \"cluster\":\"@cluster\" }\n",
    "show(plot_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some of the approaches to do Topic Modeling:\n",
    "- **Latent Dirichlet Allocation**\n",
    "- Hierarchical Dirichlet process (HDP)\n",
    "- Gibbs Sampling Dirichlet Mixture Model (GSDMM)\n",
    "- Nonnegative Matrix Factorization (NMF)\n",
    "- Latent Semantic Analysis (LSA/LSI)\n",
    "- Probabilistic Latent Semantic Analysis (pLSA)\n",
    "- Explicit semantic analysis (ESA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 **Latent Dirichlet Allocation**\n",
    "\n",
    "Latent Dirichlet Allocation (LDA) is an algorithms used to discover the topics that are present in a corpus.\n",
    "\n",
    ">  LDA starts from a fixed number of topics. Each topic is represented as a distribution over words, and each document is then represented as a distribution over topics. Although the tokens themselves are meaningless, the probability distributions over words provided by the topics provide a sense of the different ideas contained in the documents.\n",
    "> \n",
    "> Reference: https://medium.com/intuitionmachine/the-two-paths-from-natural-language-processing-to-artificial-intelligence-d5384ddbfc18\n",
    "\n",
    "Its input is a **bag of words**, i.e. each document represented as a row, with each columns containing the count of words in the corpus. We are going to use a powerful tool called pyLDAvis that gives us an interactive visualization for LDA. \n",
    "\n",
    "<img src=\"./resources/LDA.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to choose hyperparameters?\n",
    "\n",
    "There are primarily three Hyperparameters which directly influence the output.\n",
    "\n",
    "###### *doc_topic_prior : float, optional (default=None)*\n",
    "Prior of document topic distribution theta. If the value is None, defaults to 1 / n_components. This is called `alpha`.\n",
    "\n",
    "###### *topic_word_prior : float, optional (default=None)*\n",
    "Prior of topic word distribution `beta`. If the value is None, defaults to 1 / n_components.\n",
    "\n",
    "<img src=\"./resources/LDA_Hyperparameters.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?LatentDirichletAllocation\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvectorizer = CountVectorizer(min_df=4, max_features=180000, tokenizer=tokenize, ngram_range=(1,2))\n",
    "cvz = cvectorizer.fit_transform(dataframe_sample['text'])\n",
    "lda_model = LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20, random_state=42)\n",
    "X_topics = lda_model.fit_transform(cvz)\n",
    "\n",
    "n_top_words = 20\n",
    "topic_summaries = []\n",
    "\n",
    "topic_word = lda_model.components_  # get the topic words\n",
    "vocab = cvectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA Results\n",
    "\n",
    "LDA considers each documents consists of multiple topics.\n",
    "\n",
    "<img src=\"./resources/LDA_Results.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))\n",
    "    print('Topic {}: {}'.format(i, ' | '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to interpret the results\n",
    "\n",
    "<img src=\"./resources/LDA_Space.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce dimension to 2 using tsne\n",
    "tsne_lda = tsne_model.fit_transform(X_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnormalized = np.matrix(X_topics)\n",
    "doc_topic = unnormalized/unnormalized.sum(axis=1)\n",
    "\n",
    "lda_keys = []\n",
    "for i, tweet in enumerate(dataframe_sample['text']):\n",
    "    lda_keys += [doc_topic[i].argmax()]\n",
    "\n",
    "lda_df = pd.DataFrame(tsne_lda, columns=['x','y'])\n",
    "lda_df['description'] = dataframe_sample['text']\n",
    "lda_df['category'] = dataframe_sample['title']\n",
    "lda_df['topic'] = lda_keys\n",
    "lda_df['topic'] = lda_df['topic'].map(int)\n",
    "\n",
    "plot_lda = bp.figure(plot_width=700,\n",
    "                     plot_height=600,\n",
    "                     title=\"LDA topic visualization\",\n",
    "    tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "    x_axis_type=None, y_axis_type=None, min_border=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = ColumnDataSource(data=dict(x=lda_df['x'], y=lda_df['y'],\n",
    "                                    color=colormap[lda_keys],\n",
    "                                    description=lda_df['description'],\n",
    "                                    topic=lda_df['topic'],\n",
    "                                    category=lda_df['category']))\n",
    "\n",
    "plot_lda.scatter(source=source, x='x', y='y', color='color')\n",
    "hover = plot_kmeans.select(dict(type=HoverTool))\n",
    "hover = plot_lda.select(dict(type=HoverTool))\n",
    "hover.tooltips={\"description\":\"@description\",\n",
    "                \"topic\":\"@topic\", \"category\":\"@category\"}\n",
    "show(plot_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareLDAData():\n",
    "    data = {\n",
    "        'vocab': vocab,\n",
    "        'doc_topic_dists': doc_topic,\n",
    "        'doc_lengths': list(lda_df['len_docs']),\n",
    "        'term_frequency':cvectorizer.vocabulary_,\n",
    "        'topic_term_dists': lda_model.components_\n",
    "    } \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "\n",
    "lda_df['len_docs'] = dataframe_sample['tokens'].map(len)\n",
    "ldadata = prepareLDAData()\n",
    "pyLDAvis.enable_notebook()\n",
    "prepared_data = pyLDAvis.prepare(**ldadata)\n",
    "pyLDAvis.display(prepared_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To summarize:\n",
    "\n",
    "General Rule of Thumb while doing Topic Modeling using LDA.\n",
    "\n",
    "<img src=\"./resources/LDA_Steps.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some general Use-cases:\n",
    "\n",
    "1. Text Categorization problem where the labels given from the business are not very reliable.\n",
    "2. Recommender Engines for suggesting similar articles or books to potential customers.\n",
    "3. Text description for a fixed number of Products are given and asked to be clustered without label information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **References**\n",
    "\n",
    "1. [Topic Modeling Wikipedia Page](https://en.wikipedia.org/wiki/Topic_model)\n",
    "2. [Topic Modeling in Python: Latent Dirichlet Allocation (LDA); Author- Shashank Kapadia](https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0)\n",
    "3. [Beginners Guide to Topic Modeling in Python; Author - Shivam Bhansal](https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/)\n",
    "4. [Topic Modeling with Gensim (Python); Author - Selva Prabhakaran](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/)\n",
    "5. [Python for NLP: Topic Modeling; Author - Usman Malik](https://stackabuse.com/python-for-nlp-topic-modeling/)\n",
    "6. [Mercari Interactive EDA + Topic Modelling; Author -ThyKhuely](https://www.kaggle.com/thykhuely/mercari-interactive-eda-topic-modelling?scriptVersionId=1923301)\n",
    "7. [Topic Modeling for The New York Times News Dataset; Author - Moorissa Tjokro](https://towardsdatascience.com/topic-modeling-for-the-new-york-times-news-dataset-1f643e15caac)\n",
    "8. [LDA Topic Models (YouTube Video); Author - Andrius Knispelis](https://www.youtube.com/watch?v=3mHy4OSyRf0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    "if (code_show){\n",
    "$('div.input').hide();\n",
    "} else {\n",
    "$('div.input').show();\n",
    "}\n",
    "code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
